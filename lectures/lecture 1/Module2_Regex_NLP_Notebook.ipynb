{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86735202",
   "metadata": {},
   "source": [
    "\n",
    "# Module 2: Regular Expressions & NLP Preprocessing (Handsâ€‘On)\n",
    "\n",
    "This notebook accompanies your 1.5h lecture. It is **self-contained** (no internet or large model downloads needed) and covers:\n",
    "\n",
    "- **Regex basics**: character classes, quantifiers, anchors, groups\n",
    "- **Pattern matching & extraction**: emails, phones, hashtags, dates, URLs\n",
    "- **NLP preprocessing** (lightweight): tokenization, lowercasing, punctuation removal, simple stop word filtering, *toy* stemming/lemmatization rules\n",
    "- **Mini exercises** with answer keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088eba0d",
   "metadata": {},
   "source": [
    "## 0) Messy Text Sample"
   ]
  },
  {
   "cell_type": "code",
   "id": "71d820f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T08:50:32.999057Z",
     "start_time": "2025-10-09T08:50:32.989946Z"
    }
   },
   "source": [
    "\n",
    "messy_text = \"\"\"\n",
    "Hey there!  My nameâ€™s Anna, Iâ€™m from the UK ðŸ‡¬ðŸ‡§ and Iâ€™ve just bought 3 iPhones for 2,499.99 USD!!!  \n",
    "Can u believe it?? ðŸ˜‚  Email me at anna_92@example.com or contact@tech-review.co.uk.  \n",
    "BTW, check out my blog @ https://techstuff.blog or follow me on Twitter #TechLife #AI #Python3. \n",
    "Order ID: #2025-00458 | Call me maybe? +44-20-7946-0958 ðŸ“ž  \n",
    "P.S.  See you on 08/10/2025 ðŸ˜Ž\n",
    "\"\"\"\n",
    "print(messy_text)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey there!  My nameâ€™s Anna, Iâ€™m from the UK ðŸ‡¬ðŸ‡§ and Iâ€™ve just bought 3 iPhones for 2,499.99 USD!!!  \n",
      "Can u believe it?? ðŸ˜‚  Email me at anna_92@example.com or contact@tech-review.co.uk.  \n",
      "BTW, check out my blog @ https://techstuff.blog or follow me on Twitter #TechLife #AI #Python3. \n",
      "Order ID: #2025-00458 | Call me maybe? +44-20-7946-0958 ðŸ“ž  \n",
      "P.S.  See you on 08/10/2025 ðŸ˜Ž\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "9cc41e57",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Regular Expressions â€” Quick Primer\n",
    "\n",
    "**Core syntax**  \n",
    "- Character classes: `[abc]`, `[a-z]`, `\\d` (digit), `\\w` (word), `\\s` (space)\n",
    "- Quantifiers: `*` (0+), `+` (1+), `?` (0/1), `{m,n}` (range)\n",
    "- Anchors: `^` (start), `$` (end), `\\b` (word boundary)\n",
    "- Groups & alternation: `( â€¦ )`, `|`\n",
    "\n",
    "We'll use the Python `re` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a813e258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T08:54:21.442509Z",
     "start_time": "2025-10-09T08:54:21.433435Z"
    }
   },
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# A helper to pretty-print matches with their start/end indices\n",
    "def show_matches(pattern, text, flags=0):\n",
    "    print(f\"Pattern: {pattern!r}\\n\")\n",
    "    for m in re.finditer(pattern, text, flags):\n",
    "        span = f\"[{m.start()}â€“{m.end()}]\"\n",
    "        print(span, repr(m.group(0)))  # $0 is the whole match\n",
    "    if not list(re.finditer(pattern, text, flags)):\n",
    "        print(\"(no matches)\")\n",
    "\n",
    "\n",
    "# Try it out: find all words that start with a capital letter\n",
    "show_matches(r\"\\b[A-Za-z]+\\b\", messy_text)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern: '\\\\b[A-Za-z]+\\\\b'\n",
      "\n",
      "[1â€“4] 'Hey'\n",
      "[5â€“10] 'there'\n",
      "[13â€“15] 'My'\n",
      "[16â€“20] 'name'\n",
      "[21â€“22] 's'\n",
      "[23â€“27] 'Anna'\n",
      "[29â€“30] 'I'\n",
      "[31â€“32] 'm'\n",
      "[33â€“37] 'from'\n",
      "[38â€“41] 'the'\n",
      "[42â€“44] 'UK'\n",
      "[48â€“51] 'and'\n",
      "[52â€“53] 'I'\n",
      "[54â€“56] 've'\n",
      "[57â€“61] 'just'\n",
      "[62â€“68] 'bought'\n",
      "[71â€“78] 'iPhones'\n",
      "[79â€“82] 'for'\n",
      "[92â€“95] 'USD'\n",
      "[101â€“104] 'Can'\n",
      "[105â€“106] 'u'\n",
      "[107â€“114] 'believe'\n",
      "[115â€“117] 'it'\n",
      "[123â€“128] 'Email'\n",
      "[129â€“131] 'me'\n",
      "[132â€“134] 'at'\n",
      "[143â€“150] 'example'\n",
      "[151â€“154] 'com'\n",
      "[155â€“157] 'or'\n",
      "[158â€“165] 'contact'\n",
      "[166â€“170] 'tech'\n",
      "[171â€“177] 'review'\n",
      "[178â€“180] 'co'\n",
      "[181â€“183] 'uk'\n",
      "[187â€“190] 'BTW'\n",
      "[192â€“197] 'check'\n",
      "[198â€“201] 'out'\n",
      "[202â€“204] 'my'\n",
      "[205â€“209] 'blog'\n",
      "[212â€“217] 'https'\n",
      "[220â€“229] 'techstuff'\n",
      "[230â€“234] 'blog'\n",
      "[235â€“237] 'or'\n",
      "[238â€“244] 'follow'\n",
      "[245â€“247] 'me'\n",
      "[248â€“250] 'on'\n",
      "[251â€“258] 'Twitter'\n",
      "[260â€“268] 'TechLife'\n",
      "[270â€“272] 'AI'\n",
      "[284â€“289] 'Order'\n",
      "[290â€“292] 'ID'\n",
      "[308â€“312] 'Call'\n",
      "[313â€“315] 'me'\n",
      "[316â€“321] 'maybe'\n",
      "[344â€“345] 'P'\n",
      "[346â€“347] 'S'\n",
      "[350â€“353] 'See'\n",
      "[354â€“357] 'you'\n",
      "[358â€“360] 'on'\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "c16707bc",
   "metadata": {},
   "source": [
    "\n",
    "### Common Patterns You Can Try\n",
    "\n",
    "- **Emails:** ``[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}``\n",
    "- **Phone (with country code, flexible):** ``\\+?\\d{1,3}[-\\s]?\\d{2,4}[-\\s]?\\d{3,4}[-\\s]?\\d{3,4}``\n",
    "- **Hashtag:** ``#\\w+``\n",
    "- **Dates (day/month/year or with dashes):** ``\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b``\n",
    "- **URL (basic):** ``https?://[A-Za-z0-9./_-]+``\n",
    "- **Numbers (ints/decimals):** ``\\d+(\\.\\d+)?``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4c81b",
   "metadata": {},
   "source": [
    "### Extraction Examples"
   ]
  },
  {
   "cell_type": "code",
   "id": "60794fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T08:55:16.820969Z",
     "start_time": "2025-10-09T08:55:16.811084Z"
    }
   },
   "source": [
    "\n",
    "patterns = {\n",
    "    \"email\": r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\",\n",
    "    \"phone\": r\"\\+?\\d{1,3}[-\\s]?\\d{2,4}[-\\s]?\\d{3,4}[-\\s]?\\d{3,4}\",\n",
    "    \"hashtag\": r\"#\\w+\",\n",
    "    \"date_dmy\": r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\",\n",
    "    \"url\": r\"https?://[A-Za-z0-9./_-]+\",\n",
    "    \"numbers\": r\"\\d+(?:\\.\\d+)?\"\n",
    "}\n",
    "\n",
    "for label, pat in patterns.items():\n",
    "    print(f\"\\n--- {label.upper()} ---\")\n",
    "    print(re.findall(pat, messy_text))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EMAIL ---\n",
      "['anna_92@example.com', 'contact@tech-review.co.uk']\n",
      "\n",
      "--- PHONE ---\n",
      "['+44-20-7946-0958']\n",
      "\n",
      "--- HASHTAG ---\n",
      "['#TechLife', '#AI', '#Python3', '#2025']\n",
      "\n",
      "--- DATE_DMY ---\n",
      "['44-20-7946', '08/10/2025']\n",
      "\n",
      "--- URL ---\n",
      "['https://techstuff.blog']\n",
      "\n",
      "--- NUMBERS ---\n",
      "['3', '2', '499.99', '92', '3', '2025', '00458', '44', '20', '7946', '0958', '08', '10', '2025']\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "e9a7d21d",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Mini Regex Exercises (Your Turn)\n",
    "\n",
    "Using `messy_text`:\n",
    "\n",
    "1. **Find all hashtags**.  \n",
    "2. **Extract the date** (day/month/year).  \n",
    "3. **Extract all email addresses**.  \n",
    "4. **Extract all numbers, including decimals**.  \n",
    "5. **Replace multiple spaces with a single space**.  \n",
    "\n",
    "> âœï¸ Write your code in the cell below. (Hints above ðŸ‘†)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a9d33335e4e4813a"
  },
  {
   "cell_type": "markdown",
   "id": "18760443",
   "metadata": {},
   "source": [
    "\n",
    "## 3) NLP Preprocessing (Lightweight, No External Downloads)\n",
    "\n",
    "We'll implement a **simple pipeline** with only Python & regex:\n",
    "- Tokenization (split on non-letters)\n",
    "- Normalization (lowercasing)\n",
    "- Stop word removal (tiny built-in list)\n",
    "- *Toy* stemming/lemmatization (rule-based, just to illustrate the concept)\n",
    "\n",
    "> For real projects, replace this section with spaCy or NLTK pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "73aa9297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T22:24:52.710086Z",
     "start_time": "2025-10-08T22:24:52.685952Z"
    }
   },
   "source": [
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Tokenization: keep alphabetic sequences, drop punctuation & digits\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"[A-Za-z]+\", text)\n",
    "\n",
    "# 2) Normalize: lowercase all tokens\n",
    "def normalize(tokens):\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "# 3) Stop words: small illustrative set (extend as needed)\n",
    "STOP_WORDS = {\n",
    "    'a','an','the','and','or','but','if','then','else','for','of','on','in','to','is','are','am','i','you','he','she',\n",
    "    'it','we','they','me','my','our','your','their','this','that','these','those','be','been','was','were','with','as',\n",
    "    'at','by','from','so','than'\n",
    "}\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in STOP_WORDS]\n",
    "\n",
    "# 4) Toy stemmer/lemmatizer: extremely simple rules (illustrative only)\n",
    "def toy_lemmatize(token):\n",
    "    # very naive rules, for classroom demonstration\n",
    "    if token.endswith('ies') and len(token) > 4:\n",
    "        return token[:-3] + 'y'     # studies -> study\n",
    "    if token.endswith('sses') and len(token) > 5:\n",
    "        return token[:-2]           # classes -> class\n",
    "    if token.endswith('s') and len(token) > 3:\n",
    "        return token[:-1]           # cats -> cat\n",
    "    if token.endswith('ing') and len(token) > 5:\n",
    "        return token[:-3]           # running -> run (not always correct)\n",
    "    if token.endswith('ed') and len(token) > 4:\n",
    "        return token[:-2]           # chased -> chase\n",
    "    return token\n",
    "\n",
    "def toy_stem(token):\n",
    "    # even simpler: chop common suffixes\n",
    "    for suf in ('ing','ed','ly','es','s'):\n",
    "        if token.endswith(suf) and len(token) > len(suf)+2:\n",
    "            return token[:-len(suf)]\n",
    "    return token\n",
    "\n",
    "def pipeline(text, use_lemma=True):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = normalize(tokens)\n",
    "    filtered = remove_stopwords(tokens)\n",
    "    if use_lemma:\n",
    "        processed = [toy_lemmatize(t) for t in filtered]\n",
    "    else:\n",
    "        processed = [toy_stem(t) for t in filtered]\n",
    "    return processed\n",
    "\n",
    "demo_tokens = pipeline(messy_text, use_lemma=True)\n",
    "print(demo_tokens[:40])\n",
    "print(\"Token count:\", len(demo_tokens))\n",
    "print(\"Top 10:\", Counter(demo_tokens).most_common(10))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'there', 'name', 's', 'anna', 'm', 'uk', 've', 'just', 'bought', 'iphone', 'usd', 'can', 'u', 'believe', 'email', 'anna', 'example', 'com', 'contact', 'tech', 'review', 'co', 'uk', 'btw', 'check', 'out', 'blog', 'http', 'techstuff', 'blog', 'follow', 'twitter', 'techlife', 'ai', 'python', 'order', 'id', 'call', 'maybe']\n",
      "Token count: 43\n",
      "Top 10: [('s', 2), ('anna', 2), ('uk', 2), ('blog', 2), ('hey', 1), ('there', 1), ('name', 1), ('m', 1), ('ve', 1), ('just', 1)]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T22:30:14.594616Z",
     "start_time": "2025-10-08T22:30:14.587554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# TODO: Your solutions here\n",
    "import re\n",
    "\n",
    "hashtags = re.findall(r\"<#\\w+>\", messy_text)\n",
    "date = re.findall(r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\", messy_text)\n",
    "emails = re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", messy_text)\n",
    "numbers = re.findall(r\"\\d+(?:\\.\\d+)?\", messy_text)\n",
    "single_spaced = re.sub(r\"\\s{2,}\", \" \", messy_text)  # replace multiple spaces with a single space\n",
    "\n",
    "print(\"hashtags:\", hashtags)\n",
    "print(\"date:\", date)\n",
    "print(\"emails:\", emails)\n",
    "print(\"numbers:\", numbers[:10], \"... (showing first 10 if many)\")\n",
    "print(\"\\n--- Single-spaced preview ---\\n\", single_spaced[:250], \"...\")\n"
   ],
   "id": "df07c423b8ea72d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags: []\n",
      "date: ['44-20-7946', '08/10/2025']\n",
      "emails: ['anna_92@example.com', 'contact@tech-review.co.uk']\n",
      "numbers: ['3', '2', '499.99', '92', '3', '2025', '00458', '44', '20', '7946'] ... (showing first 10 if many)\n",
      "\n",
      "--- Single-spaced preview ---\n",
      " \n",
      "Hey there! My nameâ€™s Anna, Iâ€™m from the UK ðŸ‡¬ðŸ‡§ and Iâ€™ve just bought 3 iPhones for 2,499.99 USD!!! Can u believe it?? ðŸ˜‚ Email me at anna_92@example.com or contact@tech-review.co.uk. BTW, check out my blog @ https://techstuff.blog or follow me on Twitt ...\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "68ebab20",
   "metadata": {},
   "source": [
    "\n",
    "### Compare Toy Stemming vs. Toy Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "34feea95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T22:28:28.403057Z",
     "start_time": "2025-10-08T22:28:28.394023Z"
    }
   },
   "source": [
    "\n",
    "sample_tokens = [\"studies\", \"running\", \"cats\", \"chased\", \"happily\", \"classes\", \"buses\", \"flying\", \"easily\", \"tries\"]\n",
    "\n",
    "print(\"Lemma head:\", [toy_lemmatize(token) for token in sample_tokens])\n",
    "print(\"Stem  head:\", [toy_stem(token) for token in sample_tokens])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma head: ['study', 'runn', 'cat', 'chas', 'happily', 'class', 'buse', 'fly', 'easily', 'try']\n",
      "Stem  head: ['studi', 'runn', 'cat', 'chas', 'happi', 'class', 'bus', 'fly', 'easi', 'tri']\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "8844d6db",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Combining Regex with NLP Cleaning\n",
    "\n",
    "Example: extract **(number, following-noun)** pairs from text, after light cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5fdd4afd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T22:30:18.695650Z",
     "start_time": "2025-10-08T22:30:18.691311Z"
    }
   },
   "source": [
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", messy_text)  # normalize whitespace\n",
    "pairs = re.findall(r\"(\\d+)\\s+([A-Za-z]+)\", clean_text)\n",
    "print(pairs)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3', 'iPhones'), ('99', 'USD')]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7bba44225b6c4588"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
